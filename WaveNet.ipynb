{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo130u1BiPP3+oOY5F3Cyk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/aNN_Audio/blob/master/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyVuHAf3sdDB"
      },
      "source": [
        "\n",
        "# WaveNet Implementation\n",
        "\n",
        "Modified WaveNet implementation with a memory of the latest receptive field in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MnNG1e_sQGs"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "class GatedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.conv_f = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.conv_g = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        padding = self.dilation - (x.shape[-1] + self.dilation - 1) % self.dilation\n",
        "        x = nn.functional.pad(x, (self.dilation, 0))\n",
        "        return torch.mul(self.conv_f(x), self.sig(self.conv_g(x)))\n",
        "\n",
        "\n",
        "class GatedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, receptive_field, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedResidualBlock, self).__init__()\n",
        "        self.receptive_field = receptive_field\n",
        "        self.gatedconv = GatedConv1d(in_channels, out_channels, kernel_size,\n",
        "                                     stride=stride, padding=padding,\n",
        "                                     dilation=dilation, groups=groups, bias=bias)\n",
        "        self.conv_1 = nn.Conv1d(out_channels, out_channels, 1, stride=1, padding=0,\n",
        "                                dilation=1, groups=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_1(self.gatedconv(x))\n",
        "        residual = torch.add(skip, x)\n",
        "\n",
        "        #skip_cut = skip.shape[-1] - self.output_width\n",
        "        #skip = skip.narrow(-1, skip_cut, self.output_width)\n",
        "        skip = skip[:, :, self.receptive_field:]\n",
        "        return residual, skip\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, num_time_samples, num_channels=1, num_classes=2 ** 16, num_blocks=2, num_layers=14,\n",
        "                 num_hidden=32, kernel_size=2):\n",
        "        super(WaveNet, self).__init__()\n",
        "        self.previous = None\n",
        "        self.num_time_samples = num_time_samples\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.num_blocks = num_blocks\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.kernel_size = kernel_size\n",
        "        self.receptive_field = (kernel_size - 1) * num_blocks * (1 + sum([2 ** k for k in range(num_layers)]))\n",
        "\n",
        "        #self.output_width = num_time_samples - self.receptive_field\n",
        "\n",
        "        print('receptive_field: {}'.format(self.receptive_field))\n",
        "        #print('Output width: {}'.format(self.output_width))\n",
        "\n",
        "        self.device = self.set_device()\n",
        "        print(self.device)\n",
        "\n",
        "        hs = []\n",
        "        batch_norms = []\n",
        "\n",
        "        # add gated convs\n",
        "        first = True\n",
        "        for b in range(num_blocks):\n",
        "            for i in range(num_layers):\n",
        "                rate = 2 ** i\n",
        "                if first:\n",
        "                    h = GatedResidualBlock(num_channels, num_hidden, kernel_size,\n",
        "                                           self.receptive_field, dilation=rate)\n",
        "                    first = False\n",
        "                else:\n",
        "                    h = GatedResidualBlock(num_hidden, num_hidden, kernel_size,\n",
        "                                           self.receptive_field, dilation=rate)\n",
        "                h.name = 'b{}-l{}'.format(b, i)\n",
        "\n",
        "                hs.append(h)\n",
        "                batch_norms.append(nn.BatchNorm1d(num_hidden))\n",
        "\n",
        "        self.hs = nn.ModuleList(hs)\n",
        "        self.batch_norms = nn.ModuleList(batch_norms)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.conv_1_1 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.conv_1_2 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.h_class = nn.Conv1d(num_hidden, num_classes, 2)\n",
        "\n",
        "        self.linear_mix = nn.Conv1d(\n",
        "            in_channels=num_hidden,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.previous is None:\n",
        "            self.previous = torch.zeros(x.shape[0], x.shape[1], self.receptive_field)\n",
        "\n",
        "        x = torch.cat((self.previous, x), dim=2)\n",
        "        self.previous = x[:, :, -self.receptive_field:]\n",
        "\n",
        "        skips = []\n",
        "        for layer, batch_norm in zip(self.hs, self.batch_norms):\n",
        "            x, skip = layer(x)\n",
        "            x = batch_norm(x)\n",
        "            skips.append(skip)\n",
        "        x = reduce(torch.add, skips)\n",
        "        #x = self.relu_1(self.conv_1_1(x))\n",
        "        #x = self.relu_2(self.conv_1_2(x))\n",
        "        return self.linear_mix(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def set_device(device=None):\n",
        "        if device is None:\n",
        "            return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            return device\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}