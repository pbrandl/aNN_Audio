{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GyVuHAf3sdDB"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHYem7o3H79wb1sVFXPgq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/aNN_Audio/blob/master/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFzhmBHtdaV"
      },
      "source": [
        "# Training Metod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpxNXcqtj_V",
        "outputId": "b5bdd256-47e0-4681-ee54-7abb7622958c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def pre_emphasis_filter(x, coeff=0.95):\n",
        "    return torch.cat((x[:, :, 0:1], x[:, :, 1:] - coeff * x[:, :, :-1]), dim=2)\n",
        "\n",
        "\n",
        "def error_to_signal(y, y_pred):\n",
        "    \"\"\"\n",
        "    Error to signal ratio with pre-emphasis filter:\n",
        "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
        "    \"\"\"\n",
        "    y, y_pred = pre_emphasis_filter(y), pre_emphasis_filter(y_pred)\n",
        "    return (y - y_pred).pow(2).sum(dim=2) / (y.pow(2).sum(dim=2) + 1e-10)\n",
        "\n",
        "\n",
        "def train(infile_x, infile_y, n_samples=2000, s_samples=18000, s_batch=5, lr=1e-3):\n",
        "    x, sr_x = torchaudio.load(infile_x, normalization=True)\n",
        "    y, sr_y = torchaudio.load(infile_y, normalization=True)\n",
        "\n",
        "    assert sr_x == sr_y, \"Expected audio data to be eqaul in sample rate.\"\n",
        "    assert x.shape == y.shape, \"Expected audio data to be eqaul in shape.\"\n",
        "\n",
        "    b_length = s_batch * s_samples\n",
        "\n",
        "    assert n_samples * s_samples * s_batch <= x.shape[1], \"Samples must not exceed audio data length.\"\n",
        "\n",
        "    model = WaveNet(s_samples, num_layers=11, num_hidden=4)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss(reduction='sum')\n",
        "    #loss_fn = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    loss_history = []\n",
        "    epochs = 4\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, s_samples * n_samples, b_length):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Shape batches of (Features, Batch, Input)\n",
        "            x_batch, y_batch = x[0, i:i + b_length], y[0, i:i + b_length]\n",
        "\n",
        "            x_batch = x_batch.reshape(s_batch, 1, s_samples)\n",
        "            y_batch = y_batch.reshape(s_batch, 1, s_samples)\n",
        "\n",
        "            prediction = model(x_batch)\n",
        "\n",
        "            # loss = error_to_signal(y_batch, prediction).mean()\n",
        "            print(\"pred\", prediction.shape, \"y\", y_batch.shape)\n",
        "            loss = loss_fn(prediction, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "            print(\"Epoch\", epoch, \"Loss:\", loss.item())\n",
        "\n",
        "    print(\"Duration:\", time.time() - start_time)\n",
        "\n",
        "    torch.save(model.state_dict(), \"Models/second_try\" + time.strftime(\"%y%m%d-%H%M\"))\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "\n",
        "train('Preprocessed/trimmed_x.wav', 'Preprocessed/trimmed_y.wav', n_samples=2000, s_samples=12000, s_batch=5)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6246e47e4f96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preprocessed/trimmed_x.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Preprocessed/trimmed_y.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-6246e47e4f96>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(infile_x, infile_y, n_samples, s_samples, s_batch, lr)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchaudio/backend/sox_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, out, normalization, channels_first, num_frames, offset, signalinfo, encodinginfo, filetype)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# check if valid file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found or is a directory\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# initialize output tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Preprocessed/trimmed_x.wav not found or is a directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8nxDDHuhxH"
      },
      "source": [
        "# Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4M7limyuXbr",
        "outputId": "bd385954-8614-4f56-ec1b-4e46046c12d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torchaudio\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/23/6b54106b3de029d3f10cf8debc302491c17630357449c900d6209665b302/torchaudio-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 4.6MB/s \n",
            "\u001b[?25hCollecting torch==1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/2a/58f8078744e0408619c63148f7a2e8e48cf007e4146b74d4bb67c56d161b/torch-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (776.7MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchaudio) (0.16.0)\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed torch-1.7.0 torchaudio-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyVuHAf3sdDB"
      },
      "source": [
        "\n",
        "# WaveNet Implementation\n",
        "\n",
        "Modified WaveNet implementation with a memory of the latest receptive field in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MnNG1e_sQGs"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "class GatedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.conv_f = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.conv_g = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        padding = self.dilation - (x.shape[-1] + self.dilation - 1) % self.dilation\n",
        "        x = nn.functional.pad(x, (self.dilation, 0))\n",
        "        return torch.mul(self.conv_f(x), self.sig(self.conv_g(x)))\n",
        "\n",
        "\n",
        "class GatedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, receptive_field, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedResidualBlock, self).__init__()\n",
        "        self.receptive_field = receptive_field\n",
        "        self.gatedconv = GatedConv1d(in_channels, out_channels, kernel_size,\n",
        "                                     stride=stride, padding=padding,\n",
        "                                     dilation=dilation, groups=groups, bias=bias)\n",
        "        self.conv_1 = nn.Conv1d(out_channels, out_channels, 1, stride=1, padding=0,\n",
        "                                dilation=1, groups=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_1(self.gatedconv(x))\n",
        "        residual = torch.add(skip, x)\n",
        "\n",
        "        #skip_cut = skip.shape[-1] - self.output_width\n",
        "        #skip = skip.narrow(-1, skip_cut, self.output_width)\n",
        "        skip = skip[:, :, self.receptive_field:]\n",
        "        return residual, skip\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, num_time_samples, num_channels=1, num_classes=2 ** 16, num_blocks=2, num_layers=14,\n",
        "                 num_hidden=32, kernel_size=2):\n",
        "        super(WaveNet, self).__init__()\n",
        "        self.previous = None\n",
        "        self.num_time_samples = num_time_samples\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.num_blocks = num_blocks\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.kernel_size = kernel_size\n",
        "        self.receptive_field = (kernel_size - 1) * num_blocks * (1 + sum([2 ** k for k in range(num_layers)]))\n",
        "\n",
        "        #self.output_width = num_time_samples - self.receptive_field\n",
        "\n",
        "        print('receptive_field: {}'.format(self.receptive_field))\n",
        "        #print('Output width: {}'.format(self.output_width))\n",
        "\n",
        "        self.device = self.set_device()\n",
        "        print(self.device)\n",
        "\n",
        "        hs = []\n",
        "        batch_norms = []\n",
        "\n",
        "        # add gated convs\n",
        "        first = True\n",
        "        for b in range(num_blocks):\n",
        "            for i in range(num_layers):\n",
        "                rate = 2 ** i\n",
        "                if first:\n",
        "                    h = GatedResidualBlock(num_channels, num_hidden, kernel_size,\n",
        "                                           self.receptive_field, dilation=rate)\n",
        "                    first = False\n",
        "                else:\n",
        "                    h = GatedResidualBlock(num_hidden, num_hidden, kernel_size,\n",
        "                                           self.receptive_field, dilation=rate)\n",
        "                h.name = 'b{}-l{}'.format(b, i)\n",
        "\n",
        "                hs.append(h)\n",
        "                batch_norms.append(nn.BatchNorm1d(num_hidden))\n",
        "\n",
        "        self.hs = nn.ModuleList(hs)\n",
        "        self.batch_norms = nn.ModuleList(batch_norms)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.conv_1_1 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.conv_1_2 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.h_class = nn.Conv1d(num_hidden, num_classes, 2)\n",
        "\n",
        "        self.linear_mix = nn.Conv1d(\n",
        "            in_channels=num_hidden,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.previous is None:\n",
        "            self.previous = torch.zeros(x.shape[0], x.shape[1], self.receptive_field)\n",
        "\n",
        "        x = torch.cat((self.previous, x), dim=2)\n",
        "        self.previous = x[:, :, -self.receptive_field:]\n",
        "\n",
        "        skips = []\n",
        "        for layer, batch_norm in zip(self.hs, self.batch_norms):\n",
        "            x, skip = layer(x)\n",
        "            x = batch_norm(x)\n",
        "            skips.append(skip)\n",
        "        x = reduce(torch.add, skips)\n",
        "        #x = self.relu_1(self.conv_1_1(x))\n",
        "        #x = self.relu_2(self.conv_1_2(x))\n",
        "        return self.linear_mix(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def set_device(device=None):\n",
        "        if device is None:\n",
        "            return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            return device\n"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}