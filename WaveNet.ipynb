{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "mount_file_id": "1QlYVQp5eyo-bFt1SpR68an12pntSutfK",
      "authorship_tag": "ABX9TyPoUykQ0m6O1csdrMP3PHa2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/aNN_Audio/blob/master/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPAATQzluGKX"
      },
      "source": [
        "# Global Variables and Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROFabdjOuM3S"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set Working Directory\n",
        "project_path = '/content/drive/My Drive/aNN_Colab'\n",
        "print(\"Working in {}.\".format(project_path))\n",
        "models_path = os.path.join(project_path, 'Models')\n",
        "preds_path = os.path.join(project_path, 'Predictions')\n",
        "pyenv = os.path.join(project_path, 'pyenv')\n",
        "sys.path.append(pyenv) \n",
        "\n",
        "# Select the Processing Device\n",
        "device = torch.device('cuda')\n",
        "print(\"Working on {}.\".format(device))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyVuHAf3sdDB"
      },
      "source": [
        "\n",
        "# WaveNet Implementation\n",
        "\n",
        "Modified WaveNet implementation with a memory of the latest receptive field in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MnNG1e_sQGs"
      },
      "source": [
        "class GatedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.conv_f = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.conv_g = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.pad(x, (self.dilation, 0))\n",
        "        return torch.mul(self.conv_f(x), self.sig(self.conv_g(x)))\n",
        "\n",
        "\n",
        "class GatedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedResidualBlock, self).__init__()\n",
        "        self.gatedconv = GatedConv1d(in_channels, out_channels, kernel_size,\n",
        "                                     stride=stride, padding=padding,\n",
        "                                     dilation=dilation, groups=groups, bias=bias)\n",
        "        self.conv_1 = nn.Conv1d(out_channels, out_channels, 1, stride=1, padding=0,\n",
        "                                dilation=1, groups=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_1(self.gatedconv(x))\n",
        "        residual = torch.add(skip, x)\n",
        "        return residual, skip\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, num_time_samples, num_channels=1, num_classes=2 ** 16, num_blocks=2, num_layers=14,\n",
        "                 num_hidden=32, kernel_size=2, device='cuda'):\n",
        "        super(WaveNet, self).__init__()\n",
        "\n",
        "        self.num_time_samples = num_time_samples\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.num_blocks = num_blocks\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.kernel_size = kernel_size\n",
        "        self.device = device\n",
        "\n",
        "        self.length_rf = (kernel_size - 1) * num_blocks * (1 + sum([2 ** k for k in range(num_layers)]))\n",
        "        self.previous_rf = torch.tensor([]).to(device) # Initial Receptive Field\n",
        "\n",
        "        hs = []\n",
        "        #batch_norms = []\n",
        "\n",
        "        # add gated convs\n",
        "        first = True\n",
        "        for b in range(num_blocks):\n",
        "            for i in range(num_layers):\n",
        "                rate = 2 ** i\n",
        "                if first:\n",
        "                    h = GatedResidualBlock(num_channels, num_hidden, kernel_size, dilation=rate)\n",
        "                    first = False\n",
        "                else:\n",
        "                    h = GatedResidualBlock(num_hidden, num_hidden, kernel_size, dilation=rate)\n",
        "                h.name = 'b{}-l{}'.format(b, i)\n",
        "\n",
        "                hs.append(h)\n",
        "                #batch_norms.append(nn.BatchNorm1d(num_hidden))\n",
        "\n",
        "        self.hs = nn.ModuleList(hs)\n",
        "        #self.batch_norms = nn.ModuleList(batch_norms)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.conv_1_1 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.conv_1_2 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.h_class = nn.Conv1d(num_hidden, num_classes, 2)\n",
        "\n",
        "        self.linear_mix = nn.Conv1d(\n",
        "            in_channels=num_hidden,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((self.previous_rf, x), dim=2)\n",
        "        self.previous_rf = x[:, :, -self.length_rf:]\n",
        "\n",
        "        skips = []\n",
        "        for layer in self.hs:\n",
        "            x, skip = layer(x)\n",
        "            skips.append(skip)\n",
        "        x = reduce(torch.add, skips)\n",
        "\n",
        "        return self.linear_mix(x)[:, :, self.length_rf:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFzhmBHtdaV"
      },
      "source": [
        "# Training Metod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KqPURrxtEI4"
      },
      "source": [
        "def load_data_set(file_x, file_y, channels=1, device='cuda'):\n",
        "    x, sr_x = torchaudio.load(x_file, normalization=True)\n",
        "    y, sr_y = torchaudio.load(y_file, normalization=True)\n",
        "    assert sr_x == sr_y, \"Expected audio data to be eqaul in sample rate.\"\n",
        "    assert x.shape == y.shape, \"Expected audio data to be equal in shape.\"\n",
        "    return x[0:channels, :].to(device), y[0:channels, :].to(device)\n",
        "\n",
        "def batchify(x, y, batch_size, length_sample):\n",
        "  \"\"\"Shape data to batches of (Sample, Batch, Channels, SampleLength).\"\"\"\n",
        "\n",
        "  assert batch_size * length_sample <= x.shape[1], \"Summed duration length must be less than train data.\"\n",
        "\n",
        "  n_samples = x.shape[1] // (length_sample*batch_size)\n",
        "  sum_length = n_samples * batch_size * length_sample\n",
        "  x = x[:, :sum_length].reshape(n_samples, batch_size, 1, length_sample)\n",
        "  y = y[:, :sum_length].reshape(n_samples, batch_size, 1, length_sample)\n",
        "  print(\"Generated a max. number of samples {} by batch size {}\\\n",
        "  and sample length {}\".format(n_samples, batch_size, length_sample))\n",
        "  return x, y\n",
        "\n",
        "# Files\n",
        "file_x = os.path.join(project_path, \"trimmed_x.wav\")\n",
        "y_file = os.path.join(project_path, \"trimmed_y.wav\")\n",
        "x, y = load_data_set(file_x, file_y, channels=1, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpxNXcqtj_V"
      },
      "source": [
        "# Details\n",
        "length_sample = 9000\n",
        "batch_size = 20\n",
        "num_channels = 1\n",
        "x, y = batchify(x, y, batch_size, length_sample)\n",
        "print(\"Train Data Shape: {}.\".format(x.shape))\n",
        "\n",
        "# Model\n",
        "model = WaveNet(length_sample, num_layers=11, num_hidden=4, num_blocks=1, device=device).to(device)\n",
        "model.previous_rf = torch.zeros(batch_size, 1, model.length_rf).to(device)\n",
        "print(\"Receptive Field Length: {}\".format(model.length_rf))\n",
        "\n",
        "\n",
        "def train(model, x, y, epochs, n_samples, length_sample, batch_size, lr):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    #loss_fn = nn.MSELoss(reduction='sum')\n",
        "    loss_fn = nn.L1Loss(reduction='sum')\n",
        "    loss_history = []\n",
        "    for epoch in range(epochs):\n",
        "        for x_batch, y_batch in zip(x, y):\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(x_batch)\n",
        "            loss = loss_fn(prediction, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_history.append(loss.item())\n",
        "        print(\"Epoch\", epoch, \"Loss:\", loss.item())\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "start_time = time.time()\n",
        "loss_history = train(model, x.to(device), y.to(device), epochs=20,\n",
        "      n_samples=n_samples, length_sample=length_sample, batch_size=batch_size, lr=1e-3)\n",
        "print(\"Duration:\", time.time() - start_time)\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(models_path, time.strftime(\"%y%m%d-%H%M\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGsKdYndSn1n"
      },
      "source": [
        "# Predict Sequence Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92GI8S8uTWd6"
      },
      "source": [
        "def get_latest_model(path):\n",
        "  model_files = glob.glob(os.path.join(path, '*'))\n",
        "  return sorted(model_files, key=os.path.getmtime, reverse=True)[0]\n",
        "\n",
        "def get_model(filename):\n",
        "  model_files = glob.glob(os.path.join(path, filename))\n",
        "  return sorted(model_files, key=os.path.getmtime, reverse=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od9lU1v6StSA"
      },
      "source": [
        "# Load Model by File\n",
        "model_file = get_latest_model(models_path)\n",
        "model = WaveNet(length_sample, num_layers=11, num_hidden=4, num_blocks=1, device=device).to(device)\n",
        "model.previous_rf = torch.zeros(1, 1, model.length_rf).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "x_file = os.path.join(project_path, \"Audio\", \"beat_test_raw_l.wav\")\n",
        "\n",
        "x_test, sr_x = torchaudio.load(x_file, normalization=True)\n",
        "x_test = x_test.to(device)\n",
        "print(\"input file shape\", x_test.shape)\n",
        "\n",
        "\n",
        "def predict_sequence(model, x, pred_length):\n",
        "    pad_size = pred_length - x.shape[1] % pred_length\n",
        "    x_padded = F.pad(x, (0, pad_size), mode='constant', value=0)\n",
        "\n",
        "    seq_length = x_padded.shape[1]\n",
        "    seq = torch.zeros(seq_length)\n",
        "    print(seq.shape)\n",
        "\n",
        "    for i in range(0, seq_length, pred_length):\n",
        "        x_slice = x_padded[:, i:i+pred_length].reshape(1, 1, pred_length)\n",
        "        print(x_slice.shape)\n",
        "        seq[i:i+pred_length] = model(x_slice)\n",
        "\n",
        "    return seq.squeeze()[:x.shape[1]]\n",
        "\n",
        "\n",
        "output = predict_sequence(model, x_test, 9000)\n",
        "\n",
        "print(\"Now writing to wav\", output.shape)\n",
        "torchaudio.save(os.path.join(project_path, \"Predictions\", \"test_pred.wav\"), output, sr_x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8nxDDHuhxH"
      },
      "source": [
        "# Extra Stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3VCw2LH8nQ7"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xMtRvz9v7k"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}