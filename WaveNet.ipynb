{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1QlYVQp5eyo-bFt1SpR68an12pntSutfK",
      "authorship_tag": "ABX9TyMAi6MLXRmK8UoG0tts6xoL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/aNN_Audio/blob/master/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyVuHAf3sdDB"
      },
      "source": [
        "\n",
        "# WaveNet Implementation\n",
        "\n",
        "Modified WaveNet implementation with a memory of the latest receptive field in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MnNG1e_sQGs"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "class GatedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.conv_f = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.conv_g = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        padding = self.dilation - (x.shape[-1] + self.dilation - 1) % self.dilation\n",
        "        x = nn.functional.pad(x, (self.dilation, 0))\n",
        "        return torch.mul(self.conv_f(x), self.sig(self.conv_g(x)))\n",
        "\n",
        "\n",
        "class GatedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, receptive_field, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedResidualBlock, self).__init__()\n",
        "        self.receptive_field = receptive_field\n",
        "        self.gatedconv = GatedConv1d(in_channels, out_channels, kernel_size,\n",
        "                                     stride=stride, padding=padding,\n",
        "                                     dilation=dilation, groups=groups, bias=bias)\n",
        "        self.conv_1 = nn.Conv1d(out_channels, out_channels, 1, stride=1, padding=0,\n",
        "                                dilation=1, groups=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_1(self.gatedconv(x))\n",
        "        residual = torch.add(skip, x)\n",
        "\n",
        "        #skip_cut = skip.shape[-1] - self.output_width\n",
        "        #skip = skip.narrow(-1, skip_cut, self.output_width)\n",
        "        #skip = skip[:, :, self.receptive_field:]\n",
        "        return residual, skip\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, num_time_samples, num_channels=1, num_classes=2 ** 16, num_blocks=2, num_layers=14,\n",
        "                 num_hidden=32, kernel_size=2, device='cuda'):\n",
        "        super(WaveNet, self).__init__()\n",
        "        self.previous = None\n",
        "        self.num_time_samples = num_time_samples\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.num_blocks = num_blocks\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.kernel_size = kernel_size\n",
        "        self.device = device\n",
        "\n",
        "        self.receptive_field = (kernel_size - 1) * num_blocks * (1 + sum([2 ** k for k in range(num_layers)]))\n",
        "\n",
        "        print('Receptive Field: {}'.format(self.receptive_field))\n",
        "\n",
        "        hs = []\n",
        "        #batch_norms = []\n",
        "\n",
        "        # add gated convs\n",
        "        first = True\n",
        "        for b in range(num_blocks):\n",
        "            for i in range(num_layers):\n",
        "                rate = 2 ** i\n",
        "                if first:\n",
        "                    h = GatedResidualBlock(num_channels, num_hidden, kernel_size,\n",
        "                                           self.receptive_field, dilation=rate)\n",
        "                    first = False\n",
        "                else:\n",
        "                    h = GatedResidualBlock(num_hidden, num_hidden, kernel_size,\n",
        "                                           self.receptive_field, dilation=rate)\n",
        "                h.name = 'b{}-l{}'.format(b, i)\n",
        "\n",
        "                hs.append(h)\n",
        "                #batch_norms.append(nn.BatchNorm1d(num_hidden))\n",
        "\n",
        "        self.hs = nn.ModuleList(hs)\n",
        "        #self.batch_norms = nn.ModuleList(batch_norms)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.conv_1_1 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.conv_1_2 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        self.h_class = nn.Conv1d(num_hidden, num_classes, 2)\n",
        "\n",
        "        self.linear_mix = nn.Conv1d(\n",
        "            in_channels=num_hidden,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.previous is None:\n",
        "            self.previous = torch.zeros(x.shape[0], \n",
        "                                        x.shape[1], \n",
        "                                        self.receptive_field,\n",
        "                                        device = self.device)\n",
        "\n",
        "        x = torch.cat((self.previous, x), dim=2)\n",
        "        self.previous = x[:, :, -self.receptive_field:]\n",
        "\n",
        "        skips = []\n",
        "        for layer in self.hs:\n",
        "            x, skip = layer(x)\n",
        "            #x = batch_norm(x)\n",
        "            skips.append(skip)\n",
        "        x = reduce(torch.add, skips)\n",
        "        #x = self.relu_1(self.conv_1_1(x))\n",
        "        #x = self.relu_2(self.conv_1_2(x))\n",
        "        return self.linear_mix(x)[:, :, self.receptive_field:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFzhmBHtdaV"
      },
      "source": [
        "# Training Metod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpxNXcqtj_V"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "\n",
        "device = cuda = torch.device('cuda')\n",
        "print(device)\n",
        "\n",
        "\n",
        "s_samples = 12000\n",
        "model = WaveNet(s_samples, num_layers=11, num_hidden=4, device=device).to(device)\n",
        "\n",
        "def train(model, infile_x, infile_y, epochs, n_samples, s_samples, s_batch, lr, device='cpu'):\n",
        "    x, sr_x = torchaudio.load(infile_x, normalization=True)\n",
        "    y, sr_y = torchaudio.load(infile_y, normalization=True)\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    print(type(x))\n",
        "\n",
        "    assert sr_x == sr_y, \"Expected audio data to be eqaul in sample rate.\"\n",
        "    assert x.shape == y.shape, \"Expected audio data to be eqaul in shape.\"\n",
        "\n",
        "    b_length = s_batch * s_samples\n",
        "\n",
        "    assert n_samples * s_samples * s_batch <= x.shape[1], \"Samples must not exceed audio data length.\"\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss(reduction='sum')\n",
        "    #loss_fn = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, s_samples * n_samples, b_length):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Shape batches of (Features, Batch, Input)\n",
        "            x_batch, y_batch = x[0, i:i + b_length], y[0, i:i + b_length]\n",
        "\n",
        "            x_batch = x_batch.reshape(s_batch, 1, s_samples)\n",
        "            y_batch = y_batch.reshape(s_batch, 1, s_samples)\n",
        "\n",
        "            prediction = model(x_batch)\n",
        "\n",
        "            # loss = error_to_signal(y_batch, prediction).mean()\n",
        "            print(\"pred\", prediction.shape, \"y\", y_batch.shape)\n",
        "            loss = loss_fn(prediction, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "            print(\"Epoch\", epoch, \"Loss:\", loss.item())\n",
        "\n",
        "    print(\"Duration:\", time.time() - start_time)\n",
        "\n",
        "    torch.save(model.to('cpu').state_dict(), \"/content/drive/My Drive/Colab Notebooks/Models/second_try\" + time.strftime(\"%y%m%d-%H%M\"))\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "x_file = \"/content/drive/My Drive/Colab Notebooks/trimmed_x.wav\"\n",
        "y_file = \"/content/drive/My Drive/Colab Notebooks/trimmed_y.wav\"\n",
        "train(model, x_file, y_file, epochs=20,\n",
        "      n_samples=2000, s_samples=s_samples, s_batch=5, lr=1e-3, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGsKdYndSn1n"
      },
      "source": [
        "# Predict Sequence Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od9lU1v6StSA"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "s_samples = 12000\n",
        "model = WaveNet(s_samples, num_layers=11, num_hidden=4, device=device).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/Models/second_try201029-2126\"))\n",
        "\n",
        "x_file = \"/content/drive/My Drive/Colab Notebooks/Audio/beat_test_raw_l.wav\"\n",
        "\n",
        "x_test, sr_x = torchaudio.load(x_file, normalization=True)\n",
        "x_test = x_test.to(device)\n",
        "print(\"input file shape\", x_test.shape)\n",
        "\n",
        "\n",
        "def predict_sequence(model, x, pred_length):\n",
        "    pad_size = pred_length - x.shape[1] % pred_length\n",
        "    x_padded = F.pad(x, (0, pad_size), mode='constant', value=0)\n",
        "\n",
        "    seq_length = x_padded.shape[1]\n",
        "    seq = torch.zeros(seq_length)\n",
        "\n",
        "    for i in range(0, seq_length, pred_length):\n",
        "        x = x_padded[:, i:i+pred_length].reshape(1, 1, pred_length)\n",
        "        seq[i:i+pred_length] = model(x)\n",
        "\n",
        "    return seq.squeeze()[:seq_length]\n",
        "\n",
        "\n",
        "output = predict_sequence(model, x_test, s_samples)\n",
        "\n",
        "print(\"Now writing to wav\", output.shape)\n",
        "torchaudio.save(\"/content/drive/My Drive/Colab Notebooks/Predictions/test_pred.wav\", output, sr_x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8nxDDHuhxH"
      },
      "source": [
        "# Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIamp8KZxHhA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4M7limyuXbr"
      },
      "source": [
        "!pip install torchaudio==0.6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3VCw2LH8nQ7"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xMtRvz9v7k"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}