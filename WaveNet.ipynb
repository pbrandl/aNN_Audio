{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1QlYVQp5eyo-bFt1SpR68an12pntSutfK",
      "authorship_tag": "ABX9TyO/lmXhevCAkcsDikCzQjDu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/aNN_Audio/blob/master/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPAATQzluGKX"
      },
      "source": [
        "# Global Variables and Requirements\n",
        "\n",
        "---\n",
        "\n",
        "The following codes requires your Google Drive and Colab to access some of the listed imports. Please also make sure to locally install torchaudio in your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DexjBOb8B00g"
      },
      "source": [
        "!pip install --target=\"$pyenv\" --upgrade torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROFabdjOuM3S"
      },
      "source": [
        "# Import from Global Python Enivironemnt\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from IPython.display import Audio\n",
        "from IPython.core.display import display\n",
        "\n",
        "\n",
        "# Set Working Directories\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive/aNN_Colab'\n",
        "print(\"Working in {}.\".format(project_path))\n",
        "models_path = os.path.join(project_path, 'Models')\n",
        "preds_path = os.path.join(project_path, 'Predictions')\n",
        "logger_path = os.path.join(project_path, 'log')\n",
        "\n",
        "# Import from Local Python Environment\n",
        "pyenv = os.path.join(project_path, 'pyenv')\n",
        "sys.path.append(pyenv)\n",
        "\n",
        "import torchaudio\n",
        "torchaudio.set_audio_backend(\"sox_io\")\n",
        "\n",
        "# Select the Processing Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Working on {}.\".format(device))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYqf0loGIpla"
      },
      "source": [
        "# Logger\n",
        "\n",
        "The logger class aims to write the model's parameters to a file during training to compensate for run-time disruptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDjFGyrjIr_-"
      },
      "source": [
        "class Logger():\n",
        "    def __init__(self, path):\n",
        "        self.logger_path = path\n",
        "\n",
        "    def log(self, id, epoch, model, loss):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.logger_path, str(id)))\n",
        "        except FileExistsError as e:\n",
        "            pass\n",
        "\n",
        "        with open(os.path.join(self.logger_path, str(id), str(epoch)), 'wb') as log_file:\n",
        "            torch.save(model, log_file)\n",
        "\n",
        "        with open(os.path.join(self.logger_path, str(id), 'val_loss'), 'a+') as log_file:\n",
        "            log_file.write(\"{} {}\\n\".format(epoch, loss))\n",
        "\n",
        "    def clean_log():\n",
        "        pass\n",
        "    \n",
        "    def read_log(id):\n",
        "        with open(os.path.join(logger_path, str(id), 'val_loss'), 'r') as log_file:\n",
        "            return log_file.readlines()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyVuHAf3sdDB"
      },
      "source": [
        "\n",
        "# WaveNet Implementation\n",
        "\n",
        "Modified WaveNet implementation with a memory of the latest receptive field in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MnNG1e_sQGs"
      },
      "source": [
        "class AdaptiveActivation(nn.Module):\n",
        "    \"\"\"\n",
        "        This is an adaptive activation function according to Em Karniadakis 2020.\n",
        "        Title: \"On the convergence of physics informed neural networks for linear \n",
        "        second-order elliptic and parabolic type PDEs\"\n",
        "    \"\"\"\n",
        "    def __init__(self, activation_fun):\n",
        "        super(AdaptiveActivation, self).__init__()\n",
        "        self.n = torch.Tensor([10]).to(device)\n",
        "        self.a = nn.Parameter(torch.rand(1))\n",
        "        self.activ_f = activation_fun()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.activ_f(self.n * self.a * x)\n",
        "        \n",
        "\n",
        "class GatedConv1d(nn.Module):\n",
        "    \"\"\"\n",
        "        Gated dilation layer used by WaveNet class.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.conv_f = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.conv_g = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        \n",
        "        self.tanh1 = AdaptiveActivation(nn.Tanh)\n",
        "        self.tanh2 = AdaptiveActivation(nn.Sigmoid)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.pad(x, (self.dilation, 0))\n",
        "        return torch.mul(self.tanh2(self.conv_f(x)), self.tanh1(self.conv_g(x)))\n",
        "\n",
        "\n",
        "class GatedResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "        Gated block used by WaveNet class.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedResidualBlock, self).__init__()\n",
        "        self.gatedconv = GatedConv1d(in_channels, out_channels, kernel_size,\n",
        "                                     stride=stride, padding=padding,\n",
        "                                     dilation=dilation, groups=groups, bias=bias)\n",
        "        self.conv_1 = nn.Conv1d(out_channels, out_channels, 1, stride=1, padding=0,\n",
        "                                dilation=1, groups=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_1(self.gatedconv(x))\n",
        "        residual = torch.add(skip, x)\n",
        "        return residual, skip\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, num_time_samples, num_channels=1, num_blocks=2, max_dilation=14,\n",
        "                 num_hidden=32, kernel_size=2, device='cuda'):\n",
        "        super(WaveNet, self).__init__()\n",
        "        \n",
        "        self.input_length = 0\n",
        "        self.num_time_samples = num_time_samples\n",
        "        self.num_channels = num_channels\n",
        "        self.num_blocks = num_blocks\n",
        "        self.max_dilation = max_dilation\n",
        "        self.num_hidden = num_hidden\n",
        "        self.kernel_size = kernel_size\n",
        "        self.device = device\n",
        "\n",
        "        self.length_rf = (kernel_size - 1) * num_blocks * (1+ sum([2 ** k for k in range(max_dilation)]))\n",
        "        self.previous_rf = None # Initial Receptive Field\n",
        "        self.x_shape = None # Remember the input shape\n",
        "\n",
        "        stacked_dilation = []\n",
        "\n",
        "        first = True\n",
        "        for b in range(num_blocks):\n",
        "            for i in range(max_dilation):\n",
        "                rate = 2 ** i\n",
        "                if first:\n",
        "                    hidden = GatedResidualBlock(num_channels, num_hidden, kernel_size, dilation=rate)\n",
        "                    first = False\n",
        "                else:\n",
        "                    hidden = GatedResidualBlock(num_hidden, num_hidden, kernel_size, dilation=rate)\n",
        "                    \n",
        "                hidden.name = 'b{}-l{}'.format(b, i)\n",
        "                stacked_dilation.append(hidden)\n",
        "                #stacked_dilation.append(nn.Tanh())\n",
        "                #batch_norms.append(nn.BatchNorm1d(num_hidden))\n",
        "\n",
        "        self.stacked_dilation = nn.ModuleList(stacked_dilation)\n",
        "        \n",
        "        self.atanh = AdaptiveActivation(nn.Tanh)\n",
        "\n",
        "        self.linear_mix = nn.Conv1d(\n",
        "            in_channels=num_hidden,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "        )\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    @property\n",
        "    def n_param(self):\n",
        "        # Returns the number of parameters within the net.\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def reset_previous_rf(self):\n",
        "        # Resets the receptive field.\n",
        "        self.previous_rf = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_shape = x.shape\n",
        "\n",
        "        if self.previous_rf is None:\n",
        "            self.previous_rf = torch.zeros((x.shape[0], x.shape[1], self.length_rf)).to(device)\n",
        "\n",
        "        # Concat the last receptive field from x_(i-1) to x_i\n",
        "        x_tended = torch.cat((self.previous_rf, x), dim=2)\n",
        "        self.previous_rf = x[:, :, -self.length_rf:]\n",
        "        \n",
        "        skips = []\n",
        "        for layer in self.stacked_dilation:\n",
        "            x_tended, skip = layer(x_tended)\n",
        "            skips.append(skip)\n",
        "        \n",
        "        x_tended = reduce(torch.add, skips)\n",
        "\n",
        "        return self.linear_mix(x_tended)[:, :, self.length_rf:] + x\n",
        "\n",
        "    def predict_sequence(self, x_seq):\n",
        "        \"\"\"\n",
        "            Predicts a whole sequence of audio material. x_seq has to be two-dimensional, i.e.,\n",
        "            (channels, lengths).\n",
        "        \"\"\"\n",
        "        assert x_seq.dim() == 2, \"Expected two-dimensional input shape (channels, lengths).\"\n",
        "        \n",
        "        # Initialize \n",
        "        self.reset_previous_rf()\n",
        "        x_length = self.x_shape[-1]\n",
        "        x_seq_length = x_seq.shape[-1]\n",
        "        channels = x_seq.shape[0]\n",
        "        x_seq = x_seq.reshape(1, channels, x_seq_length)\n",
        "\n",
        "        # Pad the input, s.t. it fits to the model's input expections\n",
        "        pad_size = x_length - x_seq_length % x_length\n",
        "        x_seq_padded = F.pad(x_seq, (pad_size, 0), mode='constant', value=0)\n",
        "        x_seq_padded_length = x_seq_padded.shape[-1]\n",
        "        y_seq_padded = torch.zeros_like(x_seq_padded)\n",
        "\n",
        "        for i in range(0, x_seq_length, x_length):\n",
        "            x_slice_c0 = x_seq_padded[:, 0, i:i+x_length].unsqueeze(0)\n",
        "            y_seq_padded[:, 0, i:i+x_length] = model(x_slice_c0)\n",
        "\n",
        "            if channels == 2:\n",
        "                x_slice_c1 = x_seq_padded[:, 1, i:i+x_length].unsqueeze(0)\n",
        "                y_seq_padded[:, 1, i:i+x_length] = model(x_slice_c1)\n",
        "            #print(y_seq_padded.shape)\n",
        "\n",
        "        y_seq = y_seq_padded[:, :, pad_size:]\n",
        "\n",
        "        assert x_seq.shape == y_seq.shape, \"Expected input and output to be equal in shape.\"\n",
        "        return y_seq.reshape(channels, x_seq_length)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LfE_npScRft"
      },
      "source": [
        "Test that the WaveNet has to pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adbAAgQ_BY_t"
      },
      "source": [
        "# Test WaveNet with Random Tensor\n",
        "n_dilations = 11\n",
        "model = WaveNet(5000, max_dilation=n_dilations, num_hidden=4, num_blocks=1, kernel_size=2, device=device)\n",
        "assert len(model.stacked_dilation) == n_dilations, \"Num of layers do not match number of dilations.\"\n",
        "x = torch.rand((5, 1, 6000)).to(device)\n",
        "y = model(x)\n",
        "print(model.n_param)\n",
        "assert y.shape == x.shape, \"In- and output do not match in size. Check your previous receptive field.\"\n",
        "\n",
        "# Mono\n",
        "model.predict_sequence(torch.rand(1, 8000).to(device))\n",
        "# Stereo\n",
        "model.predict_sequence(torch.rand(2, 8000).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g46JE2m6F8Bb"
      },
      "source": [
        "# Data Set\n",
        "\n",
        "Due to the huge dataset a Loader is implemted that enables preloading dataset. Once loaded all the Loader object stores the data and the rest of the notebook code can be modified without reloading. The class AudioDataset splits the data into train, test and validation parts and creates batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Y1sw_Qa6MU"
      },
      "source": [
        "class Loader():\n",
        "    def __init__(self, file_x, file_y, channels=1, device=device):\n",
        "        self.file_x = file_x\n",
        "        self.file_y = file_y\n",
        "        self.__x, self.__sr_x = torchaudio.load(file_x, normalize=True)\n",
        "        self.__y, self.__sr_y = torchaudio.load(file_y, normalize=True)\n",
        "        assert self.__sr_x == self.__sr_y, \"Expected audio data to be eqaul in sample rate.\"\n",
        "        assert self.__x.shape == self.__y.shape, \"Expected audio data to be equal in shape.\"\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self.__x, self.__y\n",
        "\n",
        "    @property\n",
        "    def input_files(self):\n",
        "        return self.file_x, self.file_y\n",
        "\n",
        "    @property\n",
        "    def sample_rate(self):\n",
        "        return self.__sr_x\n",
        "\n",
        "# Load the data from Google Drive\n",
        "# del preloader # if already loaded to avoid RAM crash\n",
        "file_x = os.path.join(project_path, \"Preprocessed_Data\", \"210421_Dataset_mono_trim_x.wav\")\n",
        "file_y = os.path.join(project_path, \"Preprocessed_Data\", \"210421_Dataset_mono_trim_x.wav\")\n",
        "preloader = Loader(file_x, file_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZEdK00mF-Z9"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_x='', file_y='', channels=1, preloader=None, device='cuda'):\n",
        "        # Load Train Data\n",
        "        if file_x == '' and file_y == '':\n",
        "            self.__x, self.__y = preloader.data\n",
        "        elif preloader is not None: \n",
        "            preloader = Loader(file_x, file_y, channels, device)\n",
        "            self.__x, self.__y = preloader.data\n",
        "        else:\n",
        "            raise Exception(\"Either preloader or input files need to be specified.\") \n",
        "\n",
        "        self.file_x, self.file_y = preloader.input_files\n",
        "\n",
        "        self.x_train = self.__x\n",
        "        self.y_train = self.__y\n",
        "        self.x_valid = None\n",
        "        self.y_valid = None\n",
        "        self.x_test = None\n",
        "        self.y_test = None\n",
        "\n",
        "        self.x_batches = self.__x\n",
        "        self.y_batches = self.__y\n",
        "        self.sample_rate = preloader.sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(x.shape[1])\n",
        "\n",
        "    def __getitem__(self, channel, idx):\n",
        "        return self.x[channel, idx]\n",
        "\n",
        "    def print_file_names(self):\n",
        "        print(\"Input File:  {}\".format(self.file_x))\n",
        "        print(\"Target File: {}\".format(self.file_y))\n",
        "\n",
        "    def batchify(self, batch_dim, length_sample, n_samples='max'):\n",
        "        \"\"\"Shape data to batches of (Sample, Batch, Channels, SampleLength).\"\"\"\n",
        "        if n_samples == 'max':\n",
        "          n_samples = self.__x.shape[1] // (length_sample*batch_dim)\n",
        "\n",
        "        sum_length = n_samples * length_sample * batch_dim\n",
        "        assert sum_length <= self.__x.shape[1], \"Summed duration length must be less than train data.\"\n",
        "\n",
        "        self.x_batches = self.__x[:, :sum_length].reshape(n_samples, batch_dim, 1, length_sample)\n",
        "        self.y_batches = self.__y[:, :sum_length].reshape(n_samples, batch_dim, 1, length_sample)\n",
        "        print(\"Reshaped to {}.\".format(self.x_batches.shape))\n",
        "        return self.x_batches, self.y_batches\n",
        "\n",
        "    def split_data(self, xs, *args):\n",
        "        assert sum([arg for arg in args]) <= 1, \"Splits must sum to 1.\"\n",
        "        n_samples = xs[0].shape[0]\n",
        "        \n",
        "        for x in xs:\n",
        "            start = 0\n",
        "            splits = []\n",
        "            for arg in args:\n",
        "                end = np.rint(n_samples * arg).astype(int) + start\n",
        "                yield x[start:end]\n",
        "                start = end\n",
        "    \n",
        "    def isnan(self):\n",
        "        if torch.isnan(self.x_train).any():\n",
        "            print(\"X contains nan.\")\n",
        "        if torch.isnan(self.y_train).any():\n",
        "            print(\"Y contains nan.\")\n",
        "        \n",
        "\n",
        "f_x, f_y = preloader.input_files\n",
        "dataset = AudioDataset(preloader=preloader, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYk7Ah9gWwgc"
      },
      "source": [
        "# Listen to DataSet\n",
        "x, y = dataset.batchify(5, 400000)\n",
        "x1, x2, x3, y1, y2, y3 = dataset.split_data((x,y), 0.5, 0.4, 0.1)\n",
        "print(x1.shape, y1.shape)\n",
        "print(x2.shape, y2.shape)\n",
        "print(x3.shape, y3.shape)\n",
        "x1.shape[0] + x2.shape[0] + x3.shape[0]\n",
        "\n",
        "sample_rate = dataset.sample_rate\n",
        "display(Audio(x1[9, 0, 0, :].cpu().numpy(), rate=sample_rate))\n",
        "Audio(x1[9, 0, 0, :].cpu().numpy(), rate=sample_rate)\n",
        "# Test Data Set for nans\n",
        "dataset.isnan()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFzhmBHtdaV"
      },
      "source": [
        "# Training Metod\n",
        "\n",
        "This section includes the train, validate and testing methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpxNXcqtj_V"
      },
      "source": [
        "# RunTime Listening Files\n",
        "listening_test_file = os.path.join(project_path, \"Audio\", \"beat_test_raw_l.wav\")\n",
        "listening_test_drums = torchaudio.load(listening_test_file, normalize=True)[0].to(device)\n",
        "print(\"Listening tensor shape\", listening_test_drums.shape)\n",
        "listening_test_file = os.path.join(project_path, \"Audio\", \"test_full_mix.wav\")\n",
        "listening_test_fmix = torchaudio.load(listening_test_file, normalize=True)[0].to(device)\n",
        "print(\"Listening tensor shape\", listening_test_fmix.shape)\n",
        "display(Audio(listening_test_fmix.cpu().numpy(), rate=44100))\n",
        "display(Audio(listening_test_drums.cpu().numpy(), rate=44100))\n",
        "\n",
        "# Hyperparameter Settings\n",
        "length_sample = 4096*2\n",
        "batch_dim = 4\n",
        "num_channels = 1\n",
        "\n",
        "dataset = AudioDataset(preloader=preloader, device=device)\n",
        "x, y = dataset.batchify(batch_dim=batch_dim, length_sample=length_sample, n_samples=5000)\n",
        "x_train, x_valid, y_train, y_valid = dataset.split_data((x, y), 0.8, 0.2)\n",
        "\n",
        "model = WaveNet(length_sample, max_dilation=11, num_hidden=1, num_blocks=1, device=device)\n",
        "print(\"Train Data Shape: {}.\".format(x.shape))\n",
        "print(\"Receptive Field Length: {}.\".format(model.length_rf))\n",
        "\n",
        "logger = Logger(logger_path)\n",
        "\n",
        "assert x_train.shape ==  y_train.shape and x_valid.shape == y_valid.shape, \"Expected equal shape.\"\n",
        "\n",
        "def train(model, x, y, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for x_batch, y_batch in zip(x, y):\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(x_batch)\n",
        "        loss = loss_fn.forward(prediction, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(loss.item())\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, x, y, loss_fn):\n",
        "    loss_history = []\n",
        "    model.eval()\n",
        "    for x_sample, y_sample in zip(x, y):\n",
        "        prediction = model(x_sample)\n",
        "        loss_history.append(loss_fn.forward(prediction, y_sample))\n",
        "    return sum(loss_history) / len(loss_history)\n",
        "\n",
        "@torch.no_grad()\n",
        "def pred_listening_test(model):\n",
        "    print(\"now predicting listening_test\")\n",
        "    y_listening_test = model.predict_sequence(listening_test_drums)\n",
        "    display(Audio(y_listening_test.cpu().numpy(), rate=44100))\n",
        "    y_listening_test = model.predict_sequence(listening_test_fmix)\n",
        "    display(Audio(y_listening_test.cpu().numpy(), rate=44100))\n",
        "\n",
        "def fit(model, x_train, y_train, x_valid, y_valid, epochs, config, logger=None):\n",
        "    assert x_train.shape == y_train.shape, \"Expected data in equal shape.\"\n",
        "    assert x_valid.shape == y_valid.shape, \"Expected data in equal shape.\"\n",
        "    lr = config['lr']\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "    loss_fn = nn.MSELoss(reduction='mean')\n",
        "    #loss_fn = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    for epoch in range(int(epochs)):\n",
        "        model.reset_previous_rf()\n",
        "        loss_train = train(model, x_train, y_train, loss_fn, optimizer)\n",
        "        loss_valid = validate(model, x_valid, y_valid, loss_fn)\n",
        "        print(\"Epoch:\", epoch, \"\\nAvg. valid. loss:\", loss_valid.item())\n",
        "        if logger is not None:\n",
        "            logger.log('test', epoch, model, loss_valid)\n",
        "    \n",
        "        pred_listening_test(model)\n",
        "\n",
        "    return loss_valid, loss_train\n",
        "\n",
        "print(fit(model, x_train, y_train, x_valid, y_valid, epochs=50, config={'lr': 1e-3}, logger=None))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jOfd3Izr-Az"
      },
      "source": [
        "torch.save(model.state_dict(), os.path.join(models_path, \"model_RN_silk_512\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teh4EQoxLc_a"
      },
      "source": [
        "# Hyperparameter Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4AUZHK6J1Ks"
      },
      "source": [
        "## BOHB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ay5ijtLmrg"
      },
      "source": [
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "import hpbandster.core.nameserver as hpns\n",
        "import hpbandster.core.result as hpres\n",
        "from hpbandster.optimizers import BOHB as BOHB\n",
        "from hpbandster.core.worker import Worker\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "\n",
        "class MyWorker(Worker):\n",
        "    def __init__(self, id, dataset, sleep_interval=0, *args, **kwargs):\n",
        "        super().__init__(run_id=id, *args, **kwargs)\n",
        "        self.sleep_interval=sleep_interval\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def compute(self, config, budget, config_id, working_directory='.'):\n",
        "        input_dim = config['input_dim']\n",
        "        batch_dim = config['batch_dim']\n",
        "        n_layers = config['n_layers']\n",
        "        n_hidden = config['n_hidden']\n",
        "        kernel_size = config['kernel_size']\n",
        "\n",
        "        # Initialise Dataset\n",
        "        x, y = self.dataset.batchify(batch_dim, input_dim, n_samples=100)\n",
        "        x_train, x_valid, y_train, y_valid = dataset.split_data((x, y), 0.8, 0.2)\n",
        "\n",
        "        assert x_train.shape == y_train.shape and x_valid.shape == y_valid.shape, \"Expected equal shapes.\"\n",
        "        print(\"AAAAAAAAAAA\", x_train.shape)\n",
        "        # Initialise Model\n",
        "        model = WaveNet(input_dim, 1, 1, num_layers=n_layers, num_hidden=n_hidden, kernel_size=kernel_size, device=device)\n",
        "        model.previous_rf = torch.zeros((batch_dim, 1, model.length_rf)).to(device)\n",
        "        \n",
        "        loss, epoch = fit(model, x_train, y_train, x_valid, y_valid, budget, config)\n",
        "\n",
        "        logger.log(id=self.run_id, epoch=epoch, model=model, loss=loss)\n",
        "\n",
        "        return {'loss': loss, 'info': 1}\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def get_configspace():\n",
        "        cs = CS.ConfigurationSpace()\n",
        "\n",
        "        # Define Parameter Search Space\n",
        "        lr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value=1e-2, log=True)\n",
        "        batch_dim = CSH.UniformIntegerHyperparameter('batch_dim', lower=1, upper=15, default_value=2)\n",
        "        input_dim = CSH.UniformIntegerHyperparameter('input_dim', lower=1000, upper=20000, default_value=18000)\n",
        "        n_layers = CSH.UniformIntegerHyperparameter('n_layers', lower=1, upper=16, default_value=9)\n",
        "        n_hidden = CSH.UniformIntegerHyperparameter('n_hidden', lower=1, upper=10, default_value=1)\n",
        "        kernel_size = CSH.UniformIntegerHyperparameter('kernel_size', lower=1, upper=2, default_value=1)\n",
        "        cs.add_hyperparameters([lr, batch_dim, input_dim, n_layers, n_hidden, kernel_size])\n",
        "\n",
        "        return cs\n",
        "\n",
        "\n",
        "dataset = AudioDataset(preloader=preloader, device=device)\n",
        "NS = hpns.NameServer(run_id='0', host='127.0.0.1', port=None)\n",
        "NS.start()\n",
        "w = MyWorker(dataset=dataset, sleep_interval = 0, nameserver='127.0.0.1', id='0')\n",
        "w.run(background=True)\n",
        "bohb = BOHB(configspace = w.get_configspace(),\n",
        "            run_id = '0', nameserver='127.0.0.1',\n",
        "            min_budget=1, max_budget=2)\n",
        "\n",
        "res = bohb.run(n_iterations=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXlZ8nEaFBpw"
      },
      "source": [
        "bohb.shutdown(shutdown_workers=True)\n",
        "NS.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aFS9HkUPYNK"
      },
      "source": [
        "model = WaveNet(2)\n",
        "logger = Logger(logger_path)\n",
        "logger.log(id=4, epoch=1, model=model, loss=8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8nxDDHuhxH"
      },
      "source": [
        "# Extra Stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPgagFCGWqlZ"
      },
      "source": [
        "!pip install --target=\"$pyenv\" zounds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj0nL4v4HnQ8"
      },
      "source": [
        "!pip install --target=\"$pyenv\" ray[tune]\n",
        "!pip install --target=\"$pyenv\" hpbandster ConfigSpace\n",
        "!pip install --target=\"$pyenv\" hpbandster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThhotElbY6i5"
      },
      "source": [
        "!pip install --target=\"$pyenv\" cdpam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD1y52FVxvDD"
      },
      "source": [
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/ray/thirdparty/redis/src/redis-server\"\n",
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/ray/gcs/gcs_server\"\n",
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/plasma/plasma_store_server\"\n",
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/ray/raylet/raylet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3VCw2LH8nQ7"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xMtRvz9v7k"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}