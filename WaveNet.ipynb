{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1QlYVQp5eyo-bFt1SpR68an12pntSutfK",
      "authorship_tag": "ABX9TyPbwkmnddH9D/ovcAN2Ma6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/aNN_Audio/blob/master/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPAATQzluGKX"
      },
      "source": [
        "# Global Variables and Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROFabdjOuM3S"
      },
      "source": [
        "# Import from Global Python Enivironemnt\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from IPython.display import Audio\n",
        "from IPython.core.display import display\n",
        "\n",
        "\n",
        "# Set Working Directories\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive/aNN_Colab'\n",
        "print(\"Working in {}.\".format(project_path))\n",
        "models_path = os.path.join(project_path, 'Models')\n",
        "preds_path = os.path.join(project_path, 'Predictions')\n",
        "logger_path = os.path.join(project_path, 'log')\n",
        "\n",
        "# Import from Local Python Environment\n",
        "pyenv = os.path.join(project_path, 'pyenv')\n",
        "sys.path.append(pyenv)\n",
        "\n",
        "import torchaudio\n",
        "torchaudio.set_audio_backend(\"sox_io\")\n",
        "\n",
        "# Select the Processing Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Working on {}.\".format(device))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYqf0loGIpla"
      },
      "source": [
        "# Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDjFGyrjIr_-"
      },
      "source": [
        "class Logger():\n",
        "    def __init__(self, path):\n",
        "        self.logger_path = path\n",
        "\n",
        "    def log(self, id, epoch, model, loss):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.logger_path, str(id)))\n",
        "        except FileExistsError as e:\n",
        "            pass\n",
        "\n",
        "        with open(os.path.join(self.logger_path, str(id), str(epoch)), 'wb') as log_file:\n",
        "            torch.save(model, log_file)\n",
        "\n",
        "        with open(os.path.join(self.logger_path, str(id), 'val_loss'), 'a+') as log_file:\n",
        "            log_file.write(\"{} {}\\n\".format(epoch, loss))\n",
        "\n",
        "    def clean_log():\n",
        "        pass\n",
        "    \n",
        "    def read_log(id):\n",
        "        with open(os.path.join(logger_path, str(id), 'val_loss'), 'r') as log_file:\n",
        "            return log_file.readlines()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyVuHAf3sdDB"
      },
      "source": [
        "\n",
        "# WaveNet Implementation\n",
        "\n",
        "Modified WaveNet implementation with a memory of the latest receptive field in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MnNG1e_sQGs"
      },
      "source": [
        "class GatedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedConv1d, self).__init__()\n",
        "        self.dilation = dilation\n",
        "        self.conv_f = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.conv_g = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                groups=groups, bias=bias)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.tanh = nn.Hardtanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.pad(x, (self.dilation, 0))\n",
        "        \n",
        "        f = self.conv_f(x)\n",
        "        return torch.mul(self.conv_f(x), self.tanh(self.conv_g(x)))\n",
        "\n",
        "\n",
        "class GatedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "                 dilation=1, groups=1, bias=True):\n",
        "        super(GatedResidualBlock, self).__init__()\n",
        "        self.gatedconv = GatedConv1d(in_channels, out_channels, kernel_size,\n",
        "                                     stride=stride, padding=padding,\n",
        "                                     dilation=dilation, groups=groups, bias=bias)\n",
        "        self.conv_1 = nn.Conv1d(out_channels, out_channels, 1, stride=1, padding=0,\n",
        "                                dilation=1, groups=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.conv_1(self.gatedconv(x))\n",
        "        residual = torch.add(skip, x)\n",
        "        return residual, skip\n",
        "\n",
        "\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(self, num_time_samples, num_channels=1, num_blocks=2, max_dilation=14,\n",
        "                 num_hidden=32, kernel_size=2, device='cuda'):\n",
        "        super(WaveNet, self).__init__()\n",
        "        \n",
        "        self.input_length = 0\n",
        "        self.num_time_samples = num_time_samples\n",
        "        self.num_channels = num_channels\n",
        "        self.num_blocks = num_blocks\n",
        "        self.max_dilation = max_dilation\n",
        "        self.num_hidden = num_hidden\n",
        "        self.kernel_size = kernel_size\n",
        "        self.device = device\n",
        "\n",
        "        self.length_rf = (kernel_size - 1) * num_blocks * (1+ sum([2 ** k for k in range(max_dilation)]))\n",
        "        self.previous_rf = None # Initial Receptive Field\n",
        "        self.x_shape = None # Remember the input shape\n",
        "\n",
        "        stacked_dilation = []\n",
        "        #batch_norms = []\n",
        "\n",
        "        first = True\n",
        "        for b in range(num_blocks):\n",
        "            for i in range(max_dilation):\n",
        "                rate = 2 ** i\n",
        "                if first:\n",
        "                    hidden = GatedResidualBlock(num_channels, num_hidden, kernel_size, dilation=rate)\n",
        "                    \n",
        "                    first = False\n",
        "                else:\n",
        "                    hidden = GatedResidualBlock(num_hidden, num_hidden, kernel_size, dilation=rate)\n",
        "                    \n",
        "                hidden.name = 'b{}-l{}'.format(b, i)\n",
        "                stacked_dilation.append(hidden)\n",
        "                #stacked_dilation.append(nn.Tanh())\n",
        "                #batch_norms.append(nn.BatchNorm1d(num_hidden))\n",
        "\n",
        "        self.stacked_dilation = nn.ModuleList(stacked_dilation)\n",
        "        #self.batch_norms = nn.ModuleList(batch_norms)\n",
        "        #self.relu_1 = nn.ReLU()\n",
        "        #self.conv_1_1 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        #self.relu_2 = nn.ReLU()\n",
        "        #self.conv_1_2 = nn.Conv1d(num_hidden, num_hidden, 1)\n",
        "        #self.h_class = nn.Conv1d(num_hidden, num_classes, 2)\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.linear_mix = nn.Conv1d(\n",
        "            in_channels=num_hidden,\n",
        "            out_channels=1,\n",
        "            kernel_size=1,\n",
        "        )\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    @property\n",
        "    def n_param(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def reset_previous_rf(self):\n",
        "        self.previous_rf = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_shape = x.shape\n",
        "\n",
        "        if self.previous_rf is None:\n",
        "            self.previous_rf = torch.zeros((x.shape[0], x.shape[1], self.length_rf)).to(device)\n",
        "\n",
        "        # Concat the last receptive field from x_(i-1) to x_i\n",
        "        x_tended = torch.cat((self.previous_rf, x), dim=2)\n",
        "        self.previous_rf = x[:, :, -self.length_rf:]\n",
        "        \n",
        "        skips = []\n",
        "        for layer in self.stacked_dilation:\n",
        "            x_tended, skip = layer(x_tended)\n",
        "            skips.append(skip)\n",
        "        \n",
        "        x_tended = reduce(torch.add, skips)\n",
        "\n",
        "        return self.linear_mix(x_tended)[:, :, self.length_rf:] + x\n",
        "\n",
        "    def predict_sequence(self, x_seq):\n",
        "        assert x_seq.dim() == 2, \"Expected two-dimensional input shape (channels, lengths).\"\n",
        "        \n",
        "        # Initialize \n",
        "        self.reset_previous_rf()\n",
        "        x_length = self.x_shape[-1]\n",
        "        x_seq_length = x_seq.shape[-1]\n",
        "        x_seq = x_seq.reshape(1, 1, x_seq_length)\n",
        "\n",
        "        # Pad the input, s.t. it fits to the model's input expections\n",
        "        pad_size = x_length - x_seq_length % x_length\n",
        "        x_seq_padded = F.pad(x_seq, (pad_size, 0), mode='constant', value=0)\n",
        "        x_seq_padded_length = x_seq_padded.shape[-1]\n",
        "        y_seq_padded = torch.zeros_like(x_seq_padded)\n",
        "\n",
        "        for i in range(0, x_seq_padded_length, x_length):\n",
        "            x_slice = x_seq_padded[:, :, i:i+x_length]\n",
        "            y_seq_padded[:, :, i:i+x_length] = model(x_slice)\n",
        "            #print(y_seq_padded.shape)\n",
        "\n",
        "        y_seq = y_seq_padded[:, :, pad_size:]\n",
        "\n",
        "        assert x_seq.shape == y_seq.shape, \"Expected input and output to be equal in shape.\"\n",
        "        return y_seq.reshape(1, x_seq_length)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adbAAgQ_BY_t"
      },
      "source": [
        "# Test WaveNet with Random Tensor\n",
        "n_dilations = 11\n",
        "model = WaveNet(5000, max_dilation=n_dilations, num_hidden=1, num_blocks=1, kernel_size=2, device=device)\n",
        "assert len(model.stacked_dilation) == n_dilations, \"Num of layers do not match number of dilations.\"\n",
        "x = torch.rand((5, 1, 6000)).to(device)\n",
        "y = model(x)\n",
        "assert y.shape == x.shape, \"In- and output do not match in size. Check your previous receptive field.\"\n",
        "\n",
        "model.predict_sequence(torch.rand(1, 8000).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g46JE2m6F8Bb"
      },
      "source": [
        "# Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Y1sw_Qa6MU"
      },
      "source": [
        "class Loader():\n",
        "    def __init__(self, file_x, file_y, channels=1, device=device):\n",
        "        self.file_x = file_x\n",
        "        self.file_y = file_y\n",
        "        x, sr_x = torchaudio.load(file_x, normalize=True)\n",
        "        y, sr_y = torchaudio.load(file_y, normalize=True)\n",
        "        assert sr_x == sr_y, \"Expected audio data to be eqaul in sample rate.\"\n",
        "        assert x.shape == y.shape, \"Expected audio data to be equal in shape.\"\n",
        "        self.__x, self.__y = x[0:channels, :].to(device), y[0:channels, :].to(device)\n",
        "        self.__sr = sr_x # Sample Rate\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self.__x, self.__y\n",
        "\n",
        "    @property\n",
        "    def input_files(self):\n",
        "        return self.file_x, self.file_y\n",
        "\n",
        "    @property\n",
        "    def sample_rate(self):\n",
        "        return self.__sr\n",
        "\n",
        "file_x = os.path.join(project_path, \"trimmed_x.wav\")\n",
        "file_y = os.path.join(project_path, \"trimmed_y.wav\")\n",
        "preloader = Loader(file_x, file_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZEdK00mF-Z9"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_x='', file_y='', channels=1, preloader=None, device='cuda'):\n",
        "        # Load Train Data\n",
        "        if file_x == '' and file_y == '':\n",
        "            self.__x, self.__y = preloader.data\n",
        "        elif preloader is not None: \n",
        "            preloader = Loader(file_x, file_y, channels, device)\n",
        "            self.__x, self.__y = preloader.data\n",
        "        else:\n",
        "            raise Exception(\"Either preloader or input files need to be specified.\") \n",
        "\n",
        "        self.file_x, self.file_y = preloader.input_files\n",
        "\n",
        "        self.x_train = self.__x\n",
        "        self.y_train = self.__y\n",
        "        self.x_valid = None\n",
        "        self.y_valid = None\n",
        "        self.x_test = None\n",
        "        self.y_test = None\n",
        "\n",
        "        self.x_batches = self.__x\n",
        "        self.y_batches = self.__y\n",
        "        self.sample_rate = preloader.sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(x.shape[1])\n",
        "\n",
        "    def __getitem__(self, channel, idx):\n",
        "        return self.x[channel, idx]\n",
        "\n",
        "    def print_file_names(self):\n",
        "        print(\"Input File:  {}\".format(self.file_x))\n",
        "        print(\"Target File: {}\".format(self.file_y))\n",
        "\n",
        "    def batchify(self, batch_dim, length_sample, n_samples='max'):\n",
        "        \"\"\"Shape data to batches of (Sample, Batch, Channels, SampleLength).\"\"\"\n",
        "        if n_samples == 'max':\n",
        "          n_samples = self.__x.shape[1] // (length_sample*batch_dim)\n",
        "\n",
        "        sum_length = n_samples * length_sample * batch_dim\n",
        "        assert sum_length <= self.__x.shape[1], \"Summed duration length must be less than train data.\"\n",
        "\n",
        "        self.x_batches = self.__x[:, :sum_length].reshape(n_samples, batch_dim, 1, length_sample)\n",
        "        self.y_batches = self.__y[:, :sum_length].reshape(n_samples, batch_dim, 1, length_sample)\n",
        "        print(\"Reshaped to {}.\".format(self.x_batches.shape))\n",
        "        return self.x_batches, self.y_batches\n",
        "\n",
        "    def split_data(self, xs, *args):\n",
        "        assert sum([arg for arg in args]) <= 1, \"Splits must sum to 1.\"\n",
        "        n_samples = xs[0].shape[0]\n",
        "        \n",
        "        for x in xs:\n",
        "            start = 0\n",
        "            splits = []\n",
        "            for arg in args:\n",
        "                end = np.rint(n_samples * arg).astype(int) + start\n",
        "                yield x[start:end]\n",
        "                start = end\n",
        "                \n",
        "\n",
        "f_x, f_y = preloader.input_files\n",
        "dataset = AudioDataset(preloader=preloader, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYk7Ah9gWwgc"
      },
      "source": [
        "# Listen to DataSet\n",
        "x, y = dataset.batchify(5, 400000)\n",
        "x1, x2, x3, y1, y2, y3 = dataset.split_data((x,y), 0.5, 0.4, 0.1)\n",
        "print(x1.shape, y1.shape)\n",
        "print(x2.shape, y2.shape)\n",
        "print(x3.shape, y3.shape)\n",
        "x1.shape[0] + x2.shape[0] + x3.shape[0]\n",
        "\n",
        "sample_rate = dataset.sample_rate\n",
        "display(Audio(x1[9, 0, 0, :].cpu().numpy(), rate=sample_rate))\n",
        "Audio(x1[9, 0, 0, :].cpu().numpy(), rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFzhmBHtdaV"
      },
      "source": [
        "# Training Metod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpxNXcqtj_V"
      },
      "source": [
        "import cdpam\n",
        "\n",
        "# RunTime Listening File\n",
        "listening_test_file = os.path.join(project_path, \"Audio\", \"beat_test_raw_l.wav\")\n",
        "listening_test = torchaudio.load(listening_test_file, normalize=True)[0].to(device)\n",
        "print(\"Listening tensor shape\", listening_test.shape)\n",
        "display(Audio(listening_test.cpu().numpy(), rate=44100))\n",
        "\n",
        "# Hyperparameter Settings\n",
        "length_sample = 9000\n",
        "batch_dim = 4\n",
        "num_channels = 1\n",
        "\n",
        "dataset = AudioDataset(preloader=preloader, device=device)\n",
        "x, y = dataset.batchify(batch_dim=batch_dim, length_sample=length_sample, n_samples=5000)\n",
        "x_train, x_valid, y_train, y_valid = dataset.split_data((x, y), 0.8, 0.2)\n",
        "\n",
        "model = WaveNet(length_sample, max_dilation=11, num_hidden=4, num_blocks=1, device=device)\n",
        "print(\"Train Data Shape: {}.\".format(x.shape))\n",
        "print(\"Receptive Field Length: {}\".format(model.length_rf))\n",
        "\n",
        "logger = Logger(logger_path)\n",
        "\n",
        "assert x_train.shape ==  y_train.shape and x_valid.shape == y_valid.shape, \"Expected equal shape.\"\n",
        "\n",
        "def train(model, x, y, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for x_batch, y_batch in zip(x, y):\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(x_batch)\n",
        "        loss = loss_fn.forward(prediction, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(loss.item())\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, x, y, loss_fn):\n",
        "    loss_history = []\n",
        "    model.eval()\n",
        "    for x_sample, y_sample in zip(x, y):\n",
        "        prediction = model(x_sample)\n",
        "        loss_history.append(loss_fn.forward(prediction, y_sample))\n",
        "    return sum(loss_history)\n",
        "\n",
        "@torch.no_grad()\n",
        "def pred_listening_test(model):\n",
        "    print(\"now predicting listening_test\")\n",
        "    y_listening_test = model.predict_sequence(listening_test)\n",
        "    display(Audio(y_listening_test.cpu().numpy(), rate=44100))\n",
        "\n",
        "def fit(model, x_train, y_train, x_valid, y_valid, epochs, config, logger=None):\n",
        "    assert x_train.shape == y_train.shape, \"Expected data in equal shape.\"\n",
        "    assert x_valid.shape == y_valid.shape, \"Expected data in equal shape.\"\n",
        "    lr = config['lr']\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "    #loss_fn = nn.MSELoss(reduction='sum')\n",
        "    #loss_fn = PerceptualLoss(scale=bark, samplerate=SR44100()).to(device)\n",
        "    loss_fn = nn.L1Loss(reduction='sum')\n",
        "    #loss_fn = cdpam.CDPAM()\n",
        "\n",
        "    for epoch in range(int(epochs)):\n",
        "        model.reset_previous_rf()\n",
        "        loss_train = train(model, x_train, y_train, loss_fn, optimizer)\n",
        "        loss_valid = validate(model, x_valid, y_valid, loss_fn)\n",
        "        print(\"Epoch:\", epoch, \"Valid loss:\", loss_valid.item())\n",
        "        if logger is not None:\n",
        "            logger.log('test', epoch, model, loss_valid)\n",
        "    \n",
        "        pred_listening_test(model)\n",
        "\n",
        "    return loss_valid, loss_train\n",
        "\n",
        "print(fit(model, x_train, y_train, x_valid, y_valid, epochs=50, config={'lr': 1e-3}, logger=None))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jOfd3Izr-Az"
      },
      "source": [
        "torch.save(model.state_dict(), os.path.join(models_path, \"model_MSE_11D\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teh4EQoxLc_a"
      },
      "source": [
        "# Hyperparameter Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4AUZHK6J1Ks"
      },
      "source": [
        "## BOHB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ay5ijtLmrg"
      },
      "source": [
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "import hpbandster.core.nameserver as hpns\n",
        "import hpbandster.core.result as hpres\n",
        "from hpbandster.optimizers import BOHB as BOHB\n",
        "from hpbandster.core.worker import Worker\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "\n",
        "class MyWorker(Worker):\n",
        "    def __init__(self, id, dataset, sleep_interval=0, *args, **kwargs):\n",
        "        super().__init__(run_id=id, *args, **kwargs)\n",
        "        self.sleep_interval=sleep_interval\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def compute(self, config, budget, config_id, working_directory='.'):\n",
        "        input_dim = config['input_dim']\n",
        "        batch_dim = config['batch_dim']\n",
        "        n_layers = config['n_layers']\n",
        "        n_hidden = config['n_hidden']\n",
        "        kernel_size = config['kernel_size']\n",
        "\n",
        "        # Initialise Dataset\n",
        "        x, y = self.dataset.batchify(batch_dim, input_dim, n_samples=100)\n",
        "        x_train, x_valid, y_train, y_valid = dataset.split_data((x, y), 0.8, 0.2)\n",
        "\n",
        "        assert x_train.shape == y_train.shape and x_valid.shape == y_valid.shape, \"Expected equal shapes.\"\n",
        "        print(\"AAAAAAAAAAA\", x_train.shape)\n",
        "        # Initialise Model\n",
        "        model = WaveNet(input_dim, 1, 1, num_layers=n_layers, num_hidden=n_hidden, kernel_size=kernel_size, device=device)\n",
        "        model.previous_rf = torch.zeros((batch_dim, 1, model.length_rf)).to(device)\n",
        "        \n",
        "        loss, epoch = fit(model, x_train, y_train, x_valid, y_valid, budget, config)\n",
        "\n",
        "        logger.log(id=self.run_id, epoch=epoch, model=model, loss=loss)\n",
        "\n",
        "        return {'loss': loss, 'info': 1}\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def get_configspace():\n",
        "        cs = CS.ConfigurationSpace()\n",
        "\n",
        "        # Define Parameter Search Space\n",
        "        lr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value=1e-2, log=True)\n",
        "        batch_dim = CSH.UniformIntegerHyperparameter('batch_dim', lower=1, upper=15, default_value=2)\n",
        "        input_dim = CSH.UniformIntegerHyperparameter('input_dim', lower=1000, upper=20000, default_value=18000)\n",
        "        n_layers = CSH.UniformIntegerHyperparameter('n_layers', lower=1, upper=16, default_value=9)\n",
        "        n_hidden = CSH.UniformIntegerHyperparameter('n_hidden', lower=1, upper=10, default_value=1)\n",
        "        kernel_size = CSH.UniformIntegerHyperparameter('kernel_size', lower=1, upper=2, default_value=1)\n",
        "        cs.add_hyperparameters([lr, batch_dim, input_dim, n_layers, n_hidden, kernel_size])\n",
        "\n",
        "        return cs\n",
        "\n",
        "\n",
        "dataset = AudioDataset(preloader=preloader, device=device)\n",
        "NS = hpns.NameServer(run_id='0', host='127.0.0.1', port=None)\n",
        "NS.start()\n",
        "w = MyWorker(dataset=dataset, sleep_interval = 0, nameserver='127.0.0.1', id='0')\n",
        "w.run(background=True)\n",
        "bohb = BOHB(configspace = w.get_configspace(),\n",
        "            run_id = '0', nameserver='127.0.0.1',\n",
        "            min_budget=1, max_budget=2)\n",
        "\n",
        "res = bohb.run(n_iterations=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXlZ8nEaFBpw"
      },
      "source": [
        "bohb.shutdown(shutdown_workers=True)\n",
        "NS.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aFS9HkUPYNK"
      },
      "source": [
        "model = WaveNet(2)\n",
        "logger = Logger(logger_path)\n",
        "logger.log(id=4, epoch=1, model=model, loss=8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGsKdYndSn1n"
      },
      "source": [
        "# Predict Sequence Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92GI8S8uTWd6"
      },
      "source": [
        "def get_latest_model(path):\n",
        "  model_files = glob.glob(os.path.join(path, '*'))\n",
        "  return sorted(model_files, key=os.path.getmtime, reverse=True)[0]\n",
        "\n",
        "def get_model(filename):\n",
        "  model_files = glob.glob(os.path.join(path, filename))\n",
        "  return sorted(model_files, key=os.path.getmtime, reverse=True)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od9lU1v6StSA"
      },
      "source": [
        "# Load Model by File\n",
        "#model_file = get_latest_model(models_path)\n",
        "#model = WaveNet(length_sample, max_dilation=11, num_hidden=4, num_blocks=1, device=device).to(device)\n",
        "#model = torch.load(os.path.join(logger_path, 'test', '2'), map_location=torch.device('cpu'))\n",
        "#model.previous_rf = torch.zeros(1, 1, model.length_rf).to(device)\n",
        "#model.load_state_dict(torch.load(model_file))\n",
        "\n",
        "\n",
        "x_file = os.path.join(project_path, \"Audio\", \"beat_test_raw_l.wav\")\n",
        "x_test, sr_x = torchaudio.load(x_file, normalize=True)\n",
        "x_test = x_test.to(device)\n",
        "print(\"input file shape\", x_test.shape)\n",
        "\n",
        "\n",
        "def predict_sequence(model, x, pred_length):\n",
        "    pad_size = pred_length - x.shape[1] % pred_length\n",
        "    x_padded = F.pad(x, (0, pad_size), mode='constant', value=0)\n",
        "\n",
        "    seq_length = x_padded.shape[1]\n",
        "    seq = torch.zeros(seq_length)\n",
        "    print(seq.shape)\n",
        "\n",
        "    for i in range(0, seq_length, pred_length):\n",
        "        x_slice = x_padded[:, i:i+pred_length].reshape(1, 1, pred_length)\n",
        "        print(x_slice.shape)\n",
        "        seq[i:i+pred_length] = model(x_slice)\n",
        "\n",
        "    return seq.squeeze()[:x.shape[1]].reshape(1, x.shape[1])\n",
        "\n",
        "\n",
        "output = predict_sequence(model, x_test, 9300)\n",
        "output.shape\n",
        "print(\"Now writing to wav\", output.shape)\n",
        "torchaudio.save(os.path.join(project_path, \"Predictions\", \"test_pred.wav\"), output, sr_x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8nxDDHuhxH"
      },
      "source": [
        "# Extra Stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPgagFCGWqlZ"
      },
      "source": [
        "!pip install --target=\"$pyenv\" zounds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj0nL4v4HnQ8"
      },
      "source": [
        "!pip install --target=\"$pyenv\" ray[tune]\n",
        "!pip install --target=\"$pyenv\" hpbandster ConfigSpace\n",
        "!pip install --target=\"$pyenv\" hpbandster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DexjBOb8B00g"
      },
      "source": [
        "!pip install --target=\"$pyenv\" --upgrade torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThhotElbY6i5"
      },
      "source": [
        "!pip install --target=\"$pyenv\" cdpam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD1y52FVxvDD"
      },
      "source": [
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/ray/thirdparty/redis/src/redis-server\"\n",
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/ray/gcs/gcs_server\"\n",
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/plasma/plasma_store_server\"\n",
        "!chmod -R 755 \"/content/drive/My Drive/aNN_Colab/pyenv/ray/core/src/ray/raylet/raylet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3VCw2LH8nQ7"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xMtRvz9v7k"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}